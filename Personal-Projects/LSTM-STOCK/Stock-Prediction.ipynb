{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4807272,
          "sourceType": "datasetVersion",
          "datasetId": 2783627
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict Stock Prices with LSTM in PyTorch**\n",
        "\n",
        "In this guided project, you will create a **basic model** to predict a stock's value using daily Open, High, Low, and Close prices. The stock market can be extremely volatile, influenced by various factors. This tutorial focuses on the following stock data parameters:\n",
        "\n",
        "| Parameter | Description                                                             |\n",
        "|-----------|-------------------------------------------------------------------------|\n",
        "| **Open**  | The stock's price at market opening.                                    |\n",
        "| **High**  | The highest trading price during market hours.                          |\n",
        "| **Low**   | The lowest trading price during market hours.                           |\n",
        "| **Close** | The stock's price at market closing. Financial institutions often use this value as the stock's daily benchmark. |\n",
        "\n",
        "\n",
        "Given the significance of the Close value as a daily benchmark, it will be the primary focus for predictions. To achieve this, we will build a model using Long Short-Term Memory (LSTM), a type of Recurrent Neural Network (RNN) adept at handling large sets of time series data.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ujtmI370fez_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "### **Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "**Recurrent Neural Networks (RNNs)** are a powerful type of neural network commonly used for tasks involving sequential data. Unlike traditional neural networks, RNNs can remember previous inputs, which helps them recognize patterns over time. This ability to retain **sequential memory** makes RNNs particularly effective for tasks like predicting stock prices or analyzing text.\n",
        "\n",
        "### **How RNNs Work**?\n",
        "\n",
        "RNNs are built using three main types of layers:\n",
        "\n",
        "- **Input Layer**: This layer receives the data for the network.\n",
        "- **Hidden Layer**: This layer **processes the information** and has loops that allow it to **pass information** from previous time steps to the next. This means the model can remember **past data while processing current inputs**.\n",
        "- **Output Layer**: This layer produces the final prediction based on the processed information.\n",
        "\n",
        "The **hidden state** of an RNN represents the information the model has stored from previous steps. The amount of information it retains can vary, depending on how long it needs to remember past data.\n",
        "\n",
        "---\n",
        "\n",
        "When training an RNN, after making a prediction, we compare it to the actual result using a **loss function**. This function helps us understand how far off our prediction was. We then use a process called **backpropagation** to adjust the model. During backpropagation, we calculate the gradients (which show how much to change each weight) and update the weights of each node in the neural network accordingly.\n",
        "\n",
        "### **Why Use RNNs?**\n",
        "\n",
        "The ability to work with sequential data makes RNNs especially useful for **time series data**, such as stock prices or weather patterns. This makes them a great choice for tasks where the order of data points is important for making accurate predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-8qCLUjpW-aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "OmJ_2fDWX6SO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[Long Short-Term Memory (LSTM)](https://ieeexplore.ieee.org/abstract/document/6795963)**\n",
        "\n",
        "Recurrent backpropagation can be quite time-consuming, particularly due to insufficient and decaying error backflow. To address this issue, a specialized type of Recurrent Neural Network (RNN) called **Long Short-Term Memory (LSTM)** is utilized. **LSTMs** are designed to track long-term dependencies by modifying the model using gradients instead of traditional backpropagation. This capability is especially advantageous when working with **time series data**. The more historical data available, the better the model can train, resulting in more accurate predictions.\n",
        "\n",
        "### **Structure of an LSTM**\n",
        "\n",
        "An LSTM network contains an internal state variable that is adjusted based on weights and biases through three primary operation gates:\n",
        "\n",
        "1. **Forget Gate**: Determines what information to discard from the previous state.\n",
        "2. **Input Gate**: Decides which new information should be added to the current state.\n",
        "3. **Output Gate**: Generates the next hidden state based on the current input and the internal state.\n",
        "\n",
        "The architecture of an LSTM relies on the **tanh** and **sigmoid** functions, which help regulate the values within the network:\n",
        "\n",
        "- The **tan(h)** function ensures values remain between -1 and 1.\n",
        "- The **sigmoid** function regulates whether data should be remembered or forgotten.\n",
        "\n",
        "### **Mathematical Representation of Each Gate**\n",
        "\n",
        "The equations for the gates are as follows:\n",
        "\n",
        "- **Forget Gate**:\n",
        "  $$f_t = \\sigma(w_f * [h_{t-1}, x_t] + b_f)$$\n",
        "\n",
        "- **Input Gate**:\n",
        "  $$i_t = \\sigma(w_i * [h_{t-1}, x_t] + b_i)$$\n",
        "\n",
        "- **Output Gate**:\n",
        "  $$O_t = \\sigma(w_o * [h_{t-1}, x_t] + b_o)$$\n",
        "\n",
        "Where:  \n",
        "- $w_f$, $w_i$, $w_o$ = weight matrices for the gates\n",
        "- $h_{t-1}$ = previous hidden state\n",
        "- $x_t$ = current input\n",
        "- $b_f$, $b_i$, $b_o$ = biases for each gate\n",
        "- $\\sigma$ = sigmoid function\n",
        "\n",
        "### **Gate Functions Explained**\n",
        "\n",
        "- The **Forget Gate** determines which data to keep and which to discard.\n",
        "- The **Input Gate** analyzes what new information should be added at the current time step.\n",
        "- The **Output Gate** finalizes the updated hidden state, which will be passed to the next time step.\n",
        "\n",
        "These gates allow LSTMs to efficiently store and analyze sequential data, leading to the development of accurate predictive models.\n",
        "\n",
        "[**Source:** S. Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" in Neural Computation, vol. 9, no. 8, pp. 1735-1780, 15 Nov. 1997, doi: 10.1162/neco.1997.9.8.1735.](https://ieeexplore.ieee.org/abstract/document/6795963)\n"
      ],
      "metadata": {
        "id": "yz7u275vX7F_"
      }
    }
  ]
}